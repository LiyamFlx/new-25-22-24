<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>EngageSense: Full Features</title>
    <!-- External Libraries -->
    <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://unpkg.com/wavesurfer.js"></script>
    <script src="https://unpkg.com/meyda/dist/meyda.min.js"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css">
    <style>
        body {
            font-family: Arial, sans-serif;
            background-color: #121212;
            color: #fff;
            text-align: center;
            margin: 0;
            padding: 0;
        }
        header {
            background-color: #1f1f1f;
            padding: 10px;
            color: #00ff00;
        }
        button { margin: 10px; }
        canvas { background: black; margin: 20px auto; display: block; }
        #waveform { border: 2px solid #00ff00; }
    </style>
</head>
<body>
    <header>
        <h1>EngageSense: Real-Time Audio Engagement</h1>
    </header>

    <!-- Buttons -->
    <button id="recordBtn" class="btn btn-success">Record Audio</button>
    <button id="stopBtn" class="btn btn-danger" disabled>Stop Recording</button>
    <button id="uploadBtn" class="btn btn-info">Upload Audio</button>
    <button id="downloadBtn" class="btn btn-primary">Download Report</button>

    <!-- Real-time Audio Display -->
    <div id="waveform"></div>
    <canvas id="sentimentChart" width="400" height="200"></canvas>

    <!-- Results -->
    <div>
        <p>Loudness (RMS): <span id="rms">N/A</span></p>
        <p>Spectral Centroid: <span id="centroid">N/A</span></p>
        <p>Pitch: <span id="pitch">N/A</span></p>
        <p>Sentiment: <span id="sentiment">N/A</span></p>
        <p>Engagement: <span id="engagement">N/A</span></p>
    </div>

    <script>
        // Initialize Variables
        let audioContext, analyzer, mediaRecorder, recordedChunks = [];
        let meydaAnalyzer, wavesurfer, chart;

        // Initialize WaveSurfer
        wavesurfer = WaveSurfer.create({ container: '#waveform', waveColor: 'lime', progressColor: 'green', barWidth: 2 });

        document.getElementById('recordBtn').addEventListener('click', startRecording);
        document.getElementById('stopBtn').addEventListener('click', stopRecording);
        document.getElementById('uploadBtn').addEventListener('click', () => document.getElementById('fileInput').click());
        document.getElementById('downloadBtn').addEventListener('click', downloadReport);

        // Start Recording
        async function startRecording() {
            recordedChunks = [];
            const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
            audioContext = new AudioContext();
            analyzer = audioContext.createAnalyser();
            const source = audioContext.createMediaStreamSource(stream);
            source.connect(analyzer);

            mediaRecorder = new MediaRecorder(stream);
            mediaRecorder.start();
            mediaRecorder.ondataavailable = e => recordedChunks.push(e.data);
            meydaAnalyzer = Meyda.createMeydaAnalyzer({
                audioContext, source, bufferSize: 512,
                featureExtractors: ['rms', 'spectralCentroid', 'pitch'],
                callback: updateMetrics
            });
            meydaAnalyzer.start();
            document.getElementById('recordBtn').disabled = true;
            document.getElementById('stopBtn').disabled = false;
        }

        // Stop Recording
        function stopRecording() {
            mediaRecorder.stop();
            meydaAnalyzer.stop();
            document.getElementById('recordBtn').disabled = false;
            document.getElementById('stopBtn').disabled = true;
            const blob = new Blob(recordedChunks, { type: 'audio/wav' });
            wavesurfer.loadBlob(blob);
            analyzeAudio(blob);
        }

        // Update Metrics
        function updateMetrics(features) {
            document.getElementById('rms').innerText = features.rms.toFixed(4);
            document.getElementById('centroid').innerText = features.spectralCentroid.toFixed(2);
            document.getElementById('pitch').innerText = features.pitch.toFixed(2);
        }

        // Analyze Audio
        async function analyzeAudio(blob) {
            const model = await tf.loadLayersModel('https://storage.googleapis.com/tfjs-models/sentiment_cnn_v1/model.json');
            const input = tf.tensor2d([[0.5, 0.7, 0.8]]); // Simulated
            const sentimentScore = model.predict(input).arraySync()[0];
            document.getElementById('sentiment').innerText = sentimentScore > 0.5 ? 'Positive' : 'Negative';
            updateChart(sentimentScore);
        }

        // Update Chart
        function updateChart(sentiment) {
            const ctx = document.getElementById('sentimentChart').getContext('2d');
            if (chart) chart.destroy();
            chart = new Chart(ctx, {
                type: 'bar',
                data: {
                    labels: ['Sentiment Score'],
                    datasets: [{ data: [sentiment * 100], backgroundColor: sentiment > 0.5 ? 'green' : 'red' }]
                },
                options: { responsive: true }
            });
        }

        // Download Report
        function downloadReport() {
            const data = `Loudness (RMS): ${document.getElementById('rms').innerText}\nSpectral Centroid: ${document.getElementById('centroid').innerText}\nPitch: ${document.getElementById('pitch').innerText}\nSentiment: ${document.getElementById('sentiment').innerText}`;
            const blob = new Blob([data], { type: 'text/plain' });
            const link = document.createElement('a');
            link.href = URL.createObjectURL(blob);
            link.download = 'engagement_report.txt';
            link.click();
        }
    </script>
</body>
</html>
